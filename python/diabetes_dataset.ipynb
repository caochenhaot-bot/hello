//1
# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')  # Use Agg backend for matplotlib
import matplotlib.pyplot as plt
plt.switch_backend('Agg')  # Ensure using Agg backend if only plt is imported
%matplotlib inline  
import seaborn as sns

# Additional libraries for predictive modeling
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc
import math

# Set plot styles
sns.set(style='whitegrid')

//2
# Data Loading
df = pd.read_csv('D:\code\Software-Cup-master\python\data\diabetes_dataset.csv', delimiter=',')
print('Dataset loaded with shape:', df.shape)
df.head()

//3
# Data Cleaning and Preprocessing
print('Missing values per column:')
print(df.isnull().sum())

# If there are missing values, drop or impute them
df_clean = df.dropna()
print('After dropping missing values, shape:', df_clean.shape)

# Convert any date columns if needed (none are provided in this dataset) but this is a placeholder for future datasets
for col in df_clean.columns:
    if 'date' in col.lower():
        df_clean[col] = pd.to_datetime(df_clean[col])

# Basic cleaning: strip whitespace from string columns
str_cols = df_clean.select_dtypes(include=['object']).columns
for col in str_cols:
    df_clean[col] = df_clean[col].str.strip()

df_clean.head()

//4
# Exploratory Data Analysis
print('Dataframe Info:')
df_clean.info()

print('\nSummary Statistics:')
df_clean.describe()

//5
# Univariate Analysis
import math

# Histograms for numeric columns: Adjusting subplot grid dimensions dynamically to avoid index errors
numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
num_plots = len(numeric_cols)
cols = 4
rows = math.ceil(num_plots / cols)

plt.figure(figsize=(cols * 4, rows * 3))
for idx, col in enumerate(numeric_cols):
    plt.subplot(rows, cols, idx + 1)
    sns.histplot(df_clean[col], kde=True, bins=30)
    plt.title(col)
plt.tight_layout()
plt.show()

# Pie (count) chart for binary outcome: diagnosed_diabetes
plt.figure(figsize=(6,4))
sns.countplot(x='diagnosed_diabetes', data=df_clean, palette='pastel')
plt.title('Frequency of Diagnosed Diabetes')
plt.show()

# Bar plot for gender distribution
plt.figure(figsize=(6,4))
sns.countplot(x='gender', data=df_clean, palette='pastel')
plt.title('Gender Distribution')
plt.show()

//6
# Univariate Analysis
import math

# Histograms for numeric columns: Adjusting subplot grid dimensions dynamically to avoid index errors
numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
num_plots = len(numeric_cols)
cols = 4
rows = math.ceil(num_plots / cols)

plt.figure(figsize=(cols * 4, rows * 3))
for idx, col in enumerate(numeric_cols):
    plt.subplot(rows, cols, idx + 1)
    sns.histplot(df_clean[col], kde=True, bins=30)
    plt.title(col)
plt.tight_layout()
plt.show()

# Pie (count) chart for binary outcome: diagnosed_diabetes
plt.figure(figsize=(6,4))
sns.countplot(x='diagnosed_diabetes', data=df_clean, palette='pastel')
plt.title('Frequency of Diagnosed Diabetes')
plt.show()

# Bar plot for gender distribution
plt.figure(figsize=(6,4))
sns.countplot(x='gender', data=df_clean, palette='pastel')
plt.title('Gender Distribution')
plt.show()

//7
# Correlation Analysis
numeric_df = df_clean.select_dtypes(include=[np.number])
if numeric_df.shape[1] >= 4:
    plt.figure(figsize=(12,10))
    corr = numeric_df.corr()
    sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
    plt.title('Correlation Heatmap of Numeric Variables')
    plt.show()
else:
    print('Not enough numeric columns for a meaningful correlation heatmap.')

//8
# Predictive Modeling
from sklearn.preprocessing import StandardScaler

# We will try to predict 'diagnosed_diabetes'. For simplicity, we use the numeric columns as features.
# Since diagnosed_diabetes is binary (0 or 1), a logistic regression model is appropriate.

# Select feature columns (dropping diagnosed_diabetes from predictors)
features = numeric_df.drop('diagnosed_diabetes', axis=1)
target = numeric_df['diagnosed_diabetes']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42, stratify=target)

# Standardize the features to improve model performance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit a logistic regression classifier
model = LogisticRegression(max_iter=1000)
model.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred = model.predict(X_test_scaled)
acc = accuracy_score(y_test, y_pred)
print('Prediction Accuracy:', acc)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ROC Curve
y_prob = model.predict_proba(X_test_scaled)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, label='AUC = {:.2f}'.format(roc_auc))
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()